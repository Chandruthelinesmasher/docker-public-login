name: SRE EKS CI/CD Pipeline

on:
  push:
    branches: ["main"]
  workflow_dispatch:

permissions:
  contents: read
  id-token: write  # Required for OIDC authentication

env:
  TF_VERSION: "1.6.6"
  AWS_REGION: us-east-2

  # App
  IMAGE_NAME: k8s-sre-monitoring-app

  # Terraform Backend
  BACKEND_BUCKET: terraform-state-sre-monitoring
  BACKEND_KEY: sre-monitoring-dev.tfstate
  BACKEND_REGION: us-east-2

  # Infrastructure
  ENVIRONMENT: dev
  CLUSTER_NAME: eks-sre-monitoring-dev

jobs:
# ==========================================================
# TERRAFORM â€“ INFRASTRUCTURE
# ==========================================================
  terraform:
    name: Terraform Infra
    runs-on: ubuntu-latest

    outputs:
      ecr_repository_url: ${{ steps.tfout.outputs.ecr_repository_url }}
      ecr_repository_name: ${{ steps.tfout.outputs.ecr_repository_name }}
      eks_cluster_name: ${{ steps.tfout.outputs.eks_cluster_name }}
      aws_region: ${{ steps.tfout.outputs.aws_region }}

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      # -----------------------------
      # AWS Authentication (OIDC)
      # -----------------------------
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-Terraform

      # Alternative: Using Access Keys (if not using OIDC)
      # - name: Configure AWS Credentials
      #   uses: aws-actions/configure-aws-credentials@v4
      #   with:
      #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     aws-region: ${{ env.AWS_REGION }}

      # -----------------------------
      # Setup Terraform
      # -----------------------------
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      # -----------------------------
      # Create S3 Backend (Idempotent)
      # -----------------------------
      - name: Setup Terraform Backend
        run: |
          # Create S3 bucket for state if it doesn't exist
          if ! aws s3 ls s3://$BACKEND_BUCKET 2>/dev/null; then
            echo "Creating S3 bucket: $BACKEND_BUCKET"
            aws s3 mb s3://$BACKEND_BUCKET --region $BACKEND_REGION
            
            # Enable versioning
            aws s3api put-bucket-versioning \
              --bucket $BACKEND_BUCKET \
              --versioning-configuration Status=Enabled
            
            # Enable encryption
            aws s3api put-bucket-encryption \
              --bucket $BACKEND_BUCKET \
              --server-side-encryption-configuration '{
                "Rules": [{
                  "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                  }
                }]
              }'
            
            # Block public access
            aws s3api put-public-access-block \
              --bucket $BACKEND_BUCKET \
              --public-access-block-configuration \
                "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
          else
            echo "S3 bucket already exists: $BACKEND_BUCKET"
          fi
          
          # Create DynamoDB table for state locking if it doesn't exist
          if ! aws dynamodb describe-table --table-name terraform-state-lock --region $BACKEND_REGION 2>/dev/null; then
            echo "Creating DynamoDB table for state locking"
            aws dynamodb create-table \
              --table-name terraform-state-lock \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region $BACKEND_REGION
          else
            echo "DynamoDB table already exists: terraform-state-lock"
          fi

      # -----------------------------
      # Terraform Init
      # -----------------------------
      - name: Terraform Init
        run: |
          cd terraform
          terraform init -upgrade \
            -backend-config="bucket=$BACKEND_BUCKET" \
            -backend-config="key=$BACKEND_KEY" \
            -backend-config="region=$BACKEND_REGION" \
            -backend-config="dynamodb_table=terraform-state-lock" \
            -backend-config="encrypt=true"

      - name: Terraform Validate
        run: |
          cd terraform
          terraform validate

      - name: Terraform Plan
        run: |
          cd terraform
          terraform plan \
            -var="aws_region=$AWS_REGION" \
            -var="environment=$ENVIRONMENT" \
            -var="cluster_name=$CLUSTER_NAME" \
            -out=tfplan

      - name: Terraform Apply
        run: |
          cd terraform
          terraform apply -auto-approve tfplan

      # -----------------------------
      # Verify ECR and EKS Integration
      # -----------------------------
      - name: Verify Infrastructure
        run: |
          echo "=========================================="
          echo "Verifying AWS Infrastructure..."
          echo "=========================================="
          
          cd terraform
          
          # Get outputs from Terraform
          ECR_REPO=$(terraform output -raw ecr_repository_url)
          EKS_CLUSTER=$(terraform output -raw eks_cluster_name)
          
          echo "âœ… ECR Repository: $ECR_REPO"
          echo "âœ… EKS Cluster: $EKS_CLUSTER"
          echo ""
          echo "EKS nodes have IAM roles that allow pulling from ECR automatically!"
          echo "No additional authentication needed for image pulls."

      - name: Export Terraform Outputs
        id: tfout
        run: |
          cd terraform
          echo "ecr_repository_url=$(terraform output -raw ecr_repository_url)" >> $GITHUB_OUTPUT
          echo "ecr_repository_name=$(terraform output -raw ecr_repository_name)" >> $GITHUB_OUTPUT
          echo "eks_cluster_name=$(terraform output -raw eks_cluster_name)" >> $GITHUB_OUTPUT
          echo "aws_region=$(terraform output -raw aws_region)" >> $GITHUB_OUTPUT

# ==========================================================
# BUILD & PUSH IMAGE TO ECR
# ==========================================================
  build-and-push:
    name: Build & Push Docker Image
    needs: terraform
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ needs.terraform.outputs.aws_region }}
          role-session-name: GitHubActions-ECR

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Verify app directory exists
        run: |
          if [ ! -d "app" ]; then
            echo "Error: app directory not found!"
            echo "Repository structure:"
            ls -la
            exit 1
          fi
          echo "App directory contents:"
          ls -la app/

      - name: Build Docker Image
        run: |
          cd app
          docker build \
            -t ${{ needs.terraform.outputs.ecr_repository_url }}:latest \
            -t ${{ needs.terraform.outputs.ecr_repository_url }}:${{ github.sha }} \
            .

      - name: Push Images to ECR
        run: |
          docker push ${{ needs.terraform.outputs.ecr_repository_url }}:latest
          docker push ${{ needs.terraform.outputs.ecr_repository_url }}:${{ github.sha }}

      - name: Scan Image for Vulnerabilities
        continue-on-error: true
        run: |
          echo "Waiting for ECR scan to complete..."
          sleep 10
          
          aws ecr describe-image-scan-findings \
            --repository-name ${{ needs.terraform.outputs.ecr_repository_name }} \
            --image-id imageTag=latest \
            --region ${{ needs.terraform.outputs.aws_region }} || echo "Scan not yet available"

# ==========================================================
# DEPLOY APP + MONITORING TO EKS
# ==========================================================
  deploy:
    name: Deploy App & Monitoring
    needs: [terraform, build-and-push]
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ needs.terraform.outputs.aws_region }}
          role-session-name: GitHubActions-EKS

      # -----------------------------
      # Setup kubectl
      # -----------------------------
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ needs.terraform.outputs.aws_region }} \
            --name ${{ needs.terraform.outputs.eks_cluster_name }}

      - name: Verify EKS Connection
        run: |
          kubectl cluster-info
          kubectl get nodes

      # -----------------------------
      # Create Namespaces
      # -----------------------------
      - name: Create Namespaces
        run: |
          kubectl create namespace app --dry-run=client -o yaml | kubectl apply -f -
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -

      # -----------------------------
      # Deploy Monitoring Stack
      # -----------------------------
      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.13.0'

      - name: Deploy Monitoring Stack (Prometheus & Grafana)
        run: |
          echo "Installing Prometheus Operator and monitoring stack..."
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          
          helm upgrade --install monitoring prometheus-community/kube-prometheus-stack \
            -n monitoring \
            --set grafana.adminPassword=admin123 \
            --set prometheus.service.type=LoadBalancer \
            --set grafana.service.type=LoadBalancer \
            --wait \
            --timeout 10m
          
          echo "Waiting for Prometheus Operator CRDs to be ready..."
          kubectl wait --for condition=established --timeout=300s \
            crd/servicemonitors.monitoring.coreos.com || echo "ServiceMonitor CRD check timed out"

      # -----------------------------
      # Deploy Application
      # -----------------------------
      - name: Deploy Application
        run: |
          echo "=========================================="
          echo "Deploying application resources..."
          echo "=========================================="
          
          # Update image reference with ECR URL
          sed -i "s|IMAGE_PLACEHOLDER|${{ needs.terraform.outputs.ecr_repository_url }}:latest|g" k8s/app/deployment.yaml
          
          # Apply application manifests
          kubectl apply -f k8s/app -n app
          
          echo ""
          echo "âœ… Application manifests deployed"
          echo ""
          
          # Give pods time to start pulling images
          echo "â³ Waiting 30 seconds for pods to start..."
          sleep 30
          
          # Check initial pod status
          echo "=========================================="
          echo "Initial Pod Status:"
          echo "=========================================="
          kubectl get pods -n app -o wide
          
          # Check for image pull errors
          POD_STATUS=$(kubectl get pods -n app -o jsonpath='{.items[0].status.containerStatuses[0].state}' 2>/dev/null || echo "{}")
          if echo "$POD_STATUS" | grep -q "ImagePullBackOff\|ErrImagePull"; then
            echo ""
            echo "âš ï¸  WARNING: Detected image pull issues. Checking pod events..."
            kubectl describe pods -l app=sre-app -n app | grep -A 10 "Events:"
            echo ""
            echo "Note: EKS nodes should have IAM roles to pull from ECR automatically."
          fi

      # -----------------------------
      # Deploy ServiceMonitor
      # -----------------------------
      - name: Deploy ServiceMonitor
        run: |
          echo "Deploying ServiceMonitor for metrics scraping..."
          
          # Check if ServiceMonitor CRD exists
          if kubectl get crd servicemonitors.monitoring.coreos.com &>/dev/null; then
            kubectl apply -f k8s/monitoring/servicemonitor.yaml
            echo "âœ… ServiceMonitor deployed successfully"
          else
            echo "âš ï¸  ServiceMonitor CRD not found. Skipping ServiceMonitor deployment."
            echo "This usually means Prometheus Operator is not fully installed yet."
          fi

      # -----------------------------
      # Verify Deployment
      # -----------------------------
      - name: Verify Deployment
        run: |
          echo "Waiting for pods to be ready..."
          kubectl wait --for=condition=ready pod -l app=sre-app -n app --timeout=300s || {
            echo "âš ï¸  Pods failed to become ready. Checking pod status..."
            kubectl get pods -n app
            echo ""
            echo "Pod Events:"
            kubectl describe pods -l app=sre-app -n app | grep -A 20 Events:
            exit 1
          }
          
          echo ""
          echo "=========================================="
          echo "=== APPLICATION PODS ==="
          echo "=========================================="
          kubectl get pods -n app -o wide
          
          echo ""
          echo "=========================================="
          echo "=== MONITORING PODS ==="
          echo "=========================================="
          kubectl get pods -n monitoring
          
          echo ""
          echo "=========================================="
          echo "=== APPLICATION SERVICES ==="
          echo "=========================================="
          kubectl get svc -n app
          
          echo ""
          echo "=========================================="
          echo "=== MONITORING SERVICES ==="
          echo "=========================================="
          kubectl get svc -n monitoring
          
          echo ""
          echo "=========================================="
          echo "=== SERVICEMONITOR STATUS ==="
          echo "=========================================="
          kubectl get servicemonitor -n app || echo "No ServiceMonitors found"
          
          echo ""
          echo "=========================================="
          echo "=== APPLICATION LOAD BALANCER DNS ==="
          echo "=========================================="
          APP_LB=$(kubectl get svc sre-app-service -n app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "pending")
          if [ "$APP_LB" != "pending" ]; then
            echo "Application URL: http://$APP_LB"
          else
            echo "LoadBalancer DNS not yet assigned (this may take a few minutes)"
            echo "Run: kubectl get svc sre-app-service -n app"
          fi
          
          echo ""
          echo "=========================================="
          echo "=== GRAFANA ACCESS ==="
          echo "=========================================="
          GRAFANA_LB=$(kubectl get svc monitoring-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "pending")
          if [ "$GRAFANA_LB" != "pending" ]; then
            echo "Grafana URL: http://$GRAFANA_LB"
            echo "Username: admin"
            echo "Password: admin123"
          else
            echo "Grafana LoadBalancer DNS pending..."
            echo "Run: kubectl get svc monitoring-grafana -n monitoring"
          fi
          
          echo ""
          echo "=========================================="
          echo "=== PROMETHEUS ACCESS ==="
          echo "=========================================="
          PROM_LB=$(kubectl get svc monitoring-kube-prometheus-prometheus -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "pending")
          if [ "$PROM_LB" != "pending" ]; then
            echo "Prometheus URL: http://$PROM_LB:9090"
          else
            echo "Prometheus LoadBalancer DNS pending..."
            echo "Run: kubectl get svc monitoring-kube-prometheus-prometheus -n monitoring"
          fi
          
          echo ""
          echo "=========================================="
          echo "ðŸŽ‰ Deployment Complete!"
          echo "=========================================="