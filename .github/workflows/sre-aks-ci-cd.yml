name: SRE EKS CI/CD Pipeline

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]
  workflow_dispatch:

permissions:
  contents: read

env:
  TF_VERSION: "1.6.6"
  AWS_REGION: us-east-2

  # App
  IMAGE_NAME: k8s-sre-monitoring-app

  # Terraform Backend
  BACKEND_BUCKET: terraform-state-sre-monitoring
  BACKEND_KEY: sre-monitoring-dev.tfstate
  BACKEND_REGION: us-east-2

  # Infrastructure
  ENVIRONMENT: dev
  CLUSTER_NAME: eks-sre-monitoring-dev

jobs:
# ==========================================================
# TERRAFORM ‚Äì INFRASTRUCTURE
# ==========================================================
  terraform:
    name: Terraform Infra
    runs-on: ubuntu-latest

    outputs:
      ecr_repository_url: ${{ steps.tfout.outputs.ecr_repository_url }}
      ecr_repository_name: ${{ steps.tfout.outputs.ecr_repository_name }}
      eks_cluster_name: ${{ steps.tfout.outputs.eks_cluster_name }}
      aws_region: ${{ steps.tfout.outputs.aws_region }}

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      # -----------------------------
      # AWS Authentication (Access Keys)
      # -----------------------------
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # -----------------------------
      # Setup Terraform
      # -----------------------------
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      # -----------------------------
      # Create S3 Backend (Idempotent)
      # -----------------------------
      - name: Setup Terraform Backend
        run: |
          # Create S3 bucket for state if it doesn't exist
          if ! aws s3 ls s3://$BACKEND_BUCKET 2>/dev/null; then
            echo "Creating S3 bucket: $BACKEND_BUCKET"
            aws s3 mb s3://$BACKEND_BUCKET --region $BACKEND_REGION
            
            # Enable versioning
            aws s3api put-bucket-versioning \
              --bucket $BACKEND_BUCKET \
              --versioning-configuration Status=Enabled
            
            # Enable encryption
            aws s3api put-bucket-encryption \
              --bucket $BACKEND_BUCKET \
              --server-side-encryption-configuration '{
                "Rules": [{
                  "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                  }
                }]
              }'
            
            # Block public access
            aws s3api put-public-access-block \
              --bucket $BACKEND_BUCKET \
              --public-access-block-configuration \
                "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
            
            echo "‚úÖ S3 bucket created successfully"
          else
            echo "‚úÖ S3 bucket already exists: $BACKEND_BUCKET"
          fi
          
          # Create DynamoDB table for state locking if it doesn't exist
          if ! aws dynamodb describe-table --table-name terraform-state-lock --region $BACKEND_REGION 2>/dev/null; then
            echo "Creating DynamoDB table for state locking"
            aws dynamodb create-table \
              --table-name terraform-state-lock \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --region $BACKEND_REGION
            
            echo "Waiting for DynamoDB table to be active..."
            aws dynamodb wait table-exists \
              --table-name terraform-state-lock \
              --region $BACKEND_REGION
            
            echo "‚úÖ DynamoDB table created successfully"
          else
            echo "‚úÖ DynamoDB table already exists: terraform-state-lock"
          fi

      # -----------------------------
      # Terraform Init
      # -----------------------------
      - name: Terraform Init
        run: |
          cd terraform
          terraform init -upgrade \
            -backend-config="bucket=$BACKEND_BUCKET" \
            -backend-config="key=$BACKEND_KEY" \
            -backend-config="region=$BACKEND_REGION" \
            -backend-config="dynamodb_table=terraform-state-lock" \
            -backend-config="encrypt=true"

      - name: Terraform Format Check
        run: |
          cd terraform
          terraform fmt -check || echo "‚ö†Ô∏è  Terraform files need formatting"

      - name: Terraform Validate
        run: |
          cd terraform
          terraform validate

      - name: Terraform Plan
        id: plan
        run: |
          cd terraform
          terraform plan \
            -var="aws_region=$AWS_REGION" \
            -var="environment=$ENVIRONMENT" \
            -var="cluster_name=$CLUSTER_NAME" \
            -out=tfplan \
            -no-color
        continue-on-error: true

      - name: Check Plan Status
        if: steps.plan.outcome == 'failure'
        run: exit 1

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          cd terraform
          terraform apply -auto-approve tfplan

      # -----------------------------
      # Verify ECR and EKS Integration
      # -----------------------------
      - name: Verify Infrastructure
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          echo "=========================================="
          echo "Verifying AWS Infrastructure..."
          echo "=========================================="
          
          cd terraform
          
          # Get outputs from Terraform
          ECR_REPO=$(terraform output -raw ecr_repository_url)
          EKS_CLUSTER=$(terraform output -raw eks_cluster_name)
          VPC_ID=$(terraform output -raw vpc_id)
          
          echo "‚úÖ ECR Repository: $ECR_REPO"
          echo "‚úÖ EKS Cluster: $EKS_CLUSTER"
          echo "‚úÖ VPC ID: $VPC_ID"
          echo ""
          echo "EKS nodes have IAM roles that allow pulling from ECR automatically!"
          echo "No additional authentication needed for image pulls."

      - name: Export Terraform Outputs
        id: tfout
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          cd terraform
          echo "ecr_repository_url=$(terraform output -raw ecr_repository_url)" >> $GITHUB_OUTPUT
          echo "ecr_repository_name=$(terraform output -raw ecr_repository_name)" >> $GITHUB_OUTPUT
          echo "eks_cluster_name=$(terraform output -raw eks_cluster_name)" >> $GITHUB_OUTPUT
          echo "aws_region=$(terraform output -raw aws_region)" >> $GITHUB_OUTPUT

# ==========================================================
# BUILD & PUSH IMAGE TO ECR
# ==========================================================
  build-and-push:
    name: Build & Push Docker Image
    needs: terraform
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      # -----------------------------
      # AWS Authentication (Access Keys)
      # -----------------------------
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ needs.terraform.outputs.aws_region }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Verify app directory exists
        run: |
          if [ ! -d "app" ]; then
            echo "‚ùå Error: app directory not found!"
            echo "Repository structure:"
            ls -la
            exit 1
          fi
          echo "‚úÖ App directory found"
          echo "App directory contents:"
          ls -la app/

      - name: Build Docker Image
        run: |
          cd app
          docker build \
            --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') \
            --build-arg VCS_REF=${{ github.sha }} \
            --cache-from ${{ needs.terraform.outputs.ecr_repository_url }}:latest \
            -t ${{ needs.terraform.outputs.ecr_repository_url }}:latest \
            -t ${{ needs.terraform.outputs.ecr_repository_url }}:${{ github.sha }} \
            -t ${{ needs.terraform.outputs.ecr_repository_url }}:${{ github.run_number }} \
            .

      - name: Push Images to ECR
        run: |
          echo "Pushing images to ECR..."
          docker push ${{ needs.terraform.outputs.ecr_repository_url }}:latest
          docker push ${{ needs.terraform.outputs.ecr_repository_url }}:${{ github.sha }}
          docker push ${{ needs.terraform.outputs.ecr_repository_url }}:${{ github.run_number }}
          echo "‚úÖ Images pushed successfully"

      - name: Scan Image for Vulnerabilities
        continue-on-error: true
        run: |
          echo "‚è≥ Waiting for ECR scan to complete..."
          sleep 15
          
          echo "Checking scan results..."
          aws ecr describe-image-scan-findings \
            --repository-name ${{ needs.terraform.outputs.ecr_repository_name }} \
            --image-id imageTag=latest \
            --region ${{ needs.terraform.outputs.aws_region }} \
            --query 'imageScanFindings.findingSeverityCounts' \
            --output table || echo "‚ö†Ô∏è  Scan not yet available"

      - name: Image Summary
        run: |
          echo "=========================================="
          echo "üì¶ Docker Image Build Summary"
          echo "=========================================="
          echo "Repository: ${{ needs.terraform.outputs.ecr_repository_url }}"
          echo "Tags:"
          echo "  - latest"
          echo "  - ${{ github.sha }}"
          echo "  - ${{ github.run_number }}"
          echo "=========================================="

# ==========================================================
# DEPLOY APP + MONITORING TO EKS
# ==========================================================
  deploy:
    name: Deploy App & Monitoring
    needs: [terraform, build-and-push]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      # -----------------------------
      # AWS Authentication (Access Keys)
      # -----------------------------
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ needs.terraform.outputs.aws_region }}

      # -----------------------------
      # Setup kubectl
      # -----------------------------
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region ${{ needs.terraform.outputs.aws_region }} \
            --name ${{ needs.terraform.outputs.eks_cluster_name }}

      - name: Verify EKS Connection
        run: |
          echo "Verifying EKS cluster connection..."
          kubectl cluster-info
          echo ""
          echo "Node Status:"
          kubectl get nodes -o wide
          echo ""
          echo "‚úÖ Successfully connected to EKS cluster"

      # -----------------------------
      # Create Namespaces
      # -----------------------------
      - name: Create Namespaces
        run: |
          echo "Creating namespaces..."
          kubectl create namespace app --dry-run=client -o yaml | kubectl apply -f -
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
          echo "‚úÖ Namespaces created/verified"

      # -----------------------------
      # Deploy Monitoring Stack
      # -----------------------------
      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.13.0'

      - name: Deploy Monitoring Stack (Prometheus & Grafana)
        run: |
          echo "=========================================="
          echo "Installing Prometheus Operator and monitoring stack..."
          echo "=========================================="
          
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          
          # Check if already installed
          if helm list -n monitoring | grep -q "monitoring"; then
            echo "Upgrading existing monitoring stack..."
            ACTION="upgrade"
          else
            echo "Installing new monitoring stack..."
            ACTION="install"
          fi
          
          helm $ACTION monitoring prometheus-community/kube-prometheus-stack \
            -n monitoring \
            --set grafana.adminPassword=admin123 \
            --set prometheus.service.type=LoadBalancer \
            --set grafana.service.type=LoadBalancer \
            --set prometheus.prometheusSpec.retention=7d \
            --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.accessModes[0]=ReadWriteOnce \
            --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=10Gi \
            --wait \
            --timeout 10m
          
          echo "‚úÖ Monitoring stack deployed successfully"
          
          echo ""
          echo "Waiting for Prometheus Operator CRDs to be ready..."
          kubectl wait --for condition=established --timeout=300s \
            crd/servicemonitors.monitoring.coreos.com || echo "‚ö†Ô∏è  ServiceMonitor CRD check timed out"

      # -----------------------------
      # Deploy Application
      # -----------------------------
      - name: Deploy Application
        run: |
          echo "=========================================="
          echo "Deploying application resources..."
          echo "=========================================="
          
          # Backup original deployment file
          cp k8s/app/deployment.yaml k8s/app/deployment.yaml.bak
          
          # Update image reference with ECR URL
          sed -i "s|IMAGE_PLACEHOLDER|${{ needs.terraform.outputs.ecr_repository_url }}:${{ github.sha }}|g" k8s/app/deployment.yaml
          
          # Show what we're deploying
          echo "Image being deployed:"
          grep "image:" k8s/app/deployment.yaml | head -1
          
          # Apply application manifests
          kubectl apply -f k8s/app -n app
          
          # Restore original file
          mv k8s/app/deployment.yaml.bak k8s/app/deployment.yaml
          
          echo ""
          echo "‚úÖ Application manifests deployed"
          echo ""
          
          # Give pods time to start pulling images
          echo "‚è≥ Waiting 30 seconds for pods to start..."
          sleep 30
          
          # Check initial pod status
          echo "=========================================="
          echo "Initial Pod Status:"
          echo "=========================================="
          kubectl get pods -n app -o wide
          
          # Check for image pull errors
          POD_STATUS=$(kubectl get pods -n app -o jsonpath='{.items[0].status.containerStatuses[0].state}' 2>/dev/null || echo "{}")
          if echo "$POD_STATUS" | grep -q "ImagePullBackOff\|ErrImagePull"; then
            echo ""
            echo "‚ö†Ô∏è  WARNING: Detected image pull issues. Checking pod events..."
            kubectl describe pods -l app=sre-app -n app | grep -A 10 "Events:"
            echo ""
            echo "Note: EKS nodes should have IAM roles to pull from ECR automatically."
          fi

      # -----------------------------
      # Deploy ServiceMonitor
      # -----------------------------
      - name: Deploy ServiceMonitor
        run: |
          echo "Deploying ServiceMonitor for metrics scraping..."
          
          # Check if ServiceMonitor CRD exists
          if kubectl get crd servicemonitors.monitoring.coreos.com &>/dev/null; then
            kubectl apply -f k8s/monitoring/servicemonitor.yaml -n app
            echo "‚úÖ ServiceMonitor deployed successfully"
            
            # Verify ServiceMonitor
            echo ""
            echo "ServiceMonitor details:"
            kubectl get servicemonitor -n app
          else
            echo "‚ö†Ô∏è  ServiceMonitor CRD not found. Skipping ServiceMonitor deployment."
            echo "This usually means Prometheus Operator is not fully installed yet."
          fi

      # -----------------------------
      # Verify Deployment
      # -----------------------------
      - name: Verify Deployment
        run: |
          echo "‚è≥ Waiting for pods to be ready..."
          kubectl wait --for=condition=ready pod -l app=sre-app -n app --timeout=300s || {
            echo "‚ö†Ô∏è  Pods failed to become ready within timeout. Checking pod status..."
            kubectl get pods -n app
            echo ""
            echo "Pod Events:"
            kubectl describe pods -l app=sre-app -n app | grep -A 20 Events:
            echo ""
            echo "Pod Logs:"
            kubectl logs -l app=sre-app -n app --tail=50 || echo "Unable to fetch logs"
            exit 1
          }
          
          echo "‚úÖ All pods are ready!"
          
          echo ""
          echo "=========================================="
          echo "=== APPLICATION PODS ==="
          echo "=========================================="
          kubectl get pods -n app -o wide
          
          echo ""
          echo "=========================================="
          echo "=== MONITORING PODS ==="
          echo "=========================================="
          kubectl get pods -n monitoring | grep -E "NAME|prometheus|grafana|operator"
          
          echo ""
          echo "=========================================="
          echo "=== APPLICATION SERVICES ==="
          echo "=========================================="
          kubectl get svc -n app
          
          echo ""
          echo "=========================================="
          echo "=== MONITORING SERVICES ==="
          echo "=========================================="
          kubectl get svc -n monitoring | grep -E "NAME|prometheus|grafana"
          
          echo ""
          echo "=========================================="
          echo "=== SERVICEMONITOR STATUS ==="
          echo "=========================================="
          kubectl get servicemonitor -n app -o wide 2>/dev/null || echo "No ServiceMonitors found"

      # -----------------------------
      # Get Access URLs
      # -----------------------------
      - name: Get Access URLs
        run: |
          echo ""
          echo "=========================================="
          echo "üåê APPLICATION ACCESS INFORMATION"
          echo "=========================================="
          echo ""
          
          # Application URL
          echo "üì± APPLICATION:"
          echo "----------------------------------------"
          APP_LB=$(kubectl get svc sre-app-service -n app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$APP_LB" ] && [ "$APP_LB" != "pending" ]; then
            echo "‚úÖ Application URL: http://$APP_LB"
          else
            echo "‚è≥ LoadBalancer DNS not yet assigned (this may take 2-5 minutes)"
            echo "   Run this command to check: kubectl get svc sre-app-service -n app"
          fi
          
          echo ""
          echo "üìä GRAFANA:"
          echo "----------------------------------------"
          GRAFANA_LB=$(kubectl get svc monitoring-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$GRAFANA_LB" ] && [ "$GRAFANA_LB" != "pending" ]; then
            echo "‚úÖ Grafana URL: http://$GRAFANA_LB"
            echo "   Username: admin"
            echo "   Password: admin123"
          else
            echo "‚è≥ LoadBalancer DNS pending..."
            echo "   Run this command to check: kubectl get svc monitoring-grafana -n monitoring"
          fi
          
          echo ""
          echo "üìà PROMETHEUS:"
          echo "----------------------------------------"
          PROM_LB=$(kubectl get svc monitoring-kube-prometheus-prometheus -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$PROM_LB" ] && [ "$PROM_LB" != "pending" ]; then
            echo "‚úÖ Prometheus URL: http://$PROM_LB:9090"
          else
            echo "‚è≥ LoadBalancer DNS pending..."
            echo "   Run this command to check: kubectl get svc monitoring-kube-prometheus-prometheus -n monitoring"
          fi
          
          echo ""
          echo "=========================================="
          echo "üéâ DEPLOYMENT COMPLETE!"
          echo "=========================================="
          echo ""
          echo "üìù Useful Commands:"
          echo "  ‚Ä¢ View app logs:        kubectl logs -f -l app=sre-app -n app"
          echo "  ‚Ä¢ Scale application:    kubectl scale deployment sre-app -n app --replicas=5"
          echo "  ‚Ä¢ Get all resources:    kubectl get all -n app"
          echo "  ‚Ä¢ Port forward Grafana: kubectl port-forward -n monitoring svc/monitoring-grafana 3000:80"
          echo ""

      # -----------------------------
      # Deployment Summary
      # -----------------------------
      - name: Deployment Summary
        if: always()
        run: |
          echo ""
          echo "=========================================="
          echo "üìã DEPLOYMENT SUMMARY"
          echo "=========================================="
          echo "Cluster: ${{ needs.terraform.outputs.eks_cluster_name }}"
          echo "Region: ${{ needs.terraform.outputs.aws_region }}"
          echo "Image: ${{ needs.terraform.outputs.ecr_repository_url }}:${{ github.sha }}"
          echo "Commit: ${{ github.sha }}"
          echo "Run Number: ${{ github.run_number }}"
          echo "Triggered by: ${{ github.actor }}"
          echo "=========================================="